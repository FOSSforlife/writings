# ChatGPT

> "A Jedi uses the Force for knowledge and defense, never for attack."
> (Yoda)

ChatGPT (and the other LLMs that have caught up to it) is an incredibly useful tool when wielded properly.

At its best, it's a sounding board, a research partner, a personal tutor, an assistant for neurodivergence, a judgment-free conversationalist, and a gateway to answers for the deep, interdisciplinary questions about the world that enter my mind throughout the day.

At its worst, it's a sycophant, a bullshit artist, and a slop generator that poisons the internet.

The difference is entirely in the **prompting**.

Everything below this line is pasted from snippets of a ChatGPT conversation I had (and don't worry, it's the only thing in this entire collection of texts that is - everything else is 100% my writing). I collected and gave it some of my favorite prompts from over the years, and told it to analyze what it is about these prompts that works so well.

---

## LLM Prompting Guide

- **Aim for depth, not trivia**
  Ask questions that require synthesis, interpretation, or theory-building rather than recall or definitions.

- **Bridge disciplines on purpose**
  Combine fields (e.g. philosophy + media, CS + psychology, art + politics) to surface insights that wouldn’t appear within a single domain.

- **Provide just enough context**
  Frame the question so the model knows _why_ you’re asking and _from what angle_, without over-constraining the answer.

- **Invite comparison and contrast**
  Prompts that ask the model to compare frameworks, ideologies, eras, or works reliably produce clearer structure and sharper insights.

- **Use multi-part scaffolding**
  Break complex inquiries into sub-questions to guide reasoning without turning the prompt into a rigid checklist.

- **Leverage cultural and concrete anchors**
  Ground abstract ideas in well-known media, historical events, or real-world systems to keep answers specific and legible.

- **Encourage reflective or philosophical framing**
  Prompts that ask about values, assumptions, tradeoffs, or epistemology unlock higher-order reasoning.

- **Allow room for creative synthesis**
  Well-written prompts leave space for the model to connect dots, propose interpretations, or generate original frameworks.

- **Iterate intentionally**
  Treat prompts as part of a dialogue: refine, narrow, or invert the question based on what the previous answer revealed.

- **Ask for perspectives, not verdicts**
  Questions that explore _how different viewpoints would respond_ tend to yield richer, less brittle insights than those seeking a single “correct” answer.

## The result

A lookup engine (like Google) assumes the question is already well-formed and the answer already exists. You ask _what_, _who_, or _when_, and the system retrieves information that has been previously articulated somewhere else. This model works well for facts, definitions, and summaries, but it breaks down the moment the problem becomes ambiguous, cross-domain, or value-laden.

A collaborator for interdisciplinary reasoning operates differently.

It assumes the question itself is **unfinished**.

Instead of treating prompts as requests for stored knowledge, your prompts treat them as **working hypotheses**. You are not asking the model to fetch an answer; you are asking it to _think with you_—to help surface assumptions, map conceptual terrain, test analogies, and explore consequences across domains that don’t usually talk to each other.

This shows up in several concrete ways:

First, the prompts don’t presuppose a single correct answer. They invite multiple frameworks, tensions, and interpretations. This mirrors how real interdisciplinary research works: philosophy rarely resolves questions cleanly; sociology trades in patterns and counterexamples; art and media communicate truths obliquely. A collaborator can hold these perspectives simultaneously instead of collapsing them into a tidy conclusion.

Second, the model is used to **externalize cognition**. By asking it to compare ideologies, reconcile philosophical tensions, or translate concepts across fields, you offload part of the cognitive labor—not to avoid thinking, but to _see your own thinking more clearly_. The model becomes a mirror that reflects structure, gaps, and latent connections you might not notice internally.

Third, the interaction is iterative rather than transactional. A lookup engine encourages one-shot queries. A collaborator encourages follow-ups, reframing, and refinement. Many of your prompts implicitly assume this: they’re designed to open a line of inquiry, not close it. Each answer becomes raw material for the next question.

Fourth, the model is treated as a **synthetic generalist**. Humans often specialize deeply and integrate slowly. LLMs do the opposite: they integrate broadly but shallowly unless guided. Your prompts exploit this strength by asking the model to traverse philosophy, politics, psychology, media, and technology in a single move—something a human researcher would normally need weeks of context switching to attempt.

Finally, this approach reframes success. The goal is not “Did I get the right answer?” but “Did this move my understanding forward?” A good response might complicate your view, introduce a productive tension, or give you a better question than the one you started with. That’s the hallmark of collaboration, not retrieval.
